{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Sending GET request to https://www.shadowfox.in/#\\n\",\n",
    "      \"Response status code: 200\\n\",\n",
    "      \"Website content fetched successfully\\n\",\n",
    "      \"Data extraction completed and saved to scraped_data.csv\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import requests\\n\",\n",
    "    \"from bs4 import BeautifulSoup\\n\",\n",
    "    \"import csv\\n\",\n",
    "    \"\\n\",\n",
    "    \"# URL of the ShadowFox website\\n\",\n",
    "    \"url = \\\"https://www.shadowfox.in/#\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Custom exception for website access errors\\n\",\n",
    "    \"class WebsiteAccessError(Exception):\\n\",\n",
    "    \"    pass\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Function to send a GET request and handle errors\\n\",\n",
    "    \"def fetch_url(url):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        print(f\\\"Sending GET request to {url}\\\")\\n\",\n",
    "    \"        response = requests.get(url)\\n\",\n",
    "    \"        print(f\\\"Response status code: {response.status_code}\\\")\\n\",\n",
    "    \"        response.raise_for_status()  # Raise an exception for non-2xx status codes\\n\",\n",
    "    \"        return response.text\\n\",\n",
    "    \"    except requests.exceptions.RequestException as e:\\n\",\n",
    "    \"        raise WebsiteAccessError(f\\\"Error fetching URL: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Function to extract data from the website\\n\",\n",
    "    \"def extract_data(html_content):\\n\",\n",
    "    \"    soup = BeautifulSoup(html_content, \\\"html.parser\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Extract the desired data\\n\",\n",
    "    \"    title = soup.find(\\\"title\\\").text\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Extract description and keywords from meta tags\\n\",\n",
    "    \"    description_meta = soup.find(\\\"meta\\\", {\\\"name\\\": \\\"description\\\"})\\n\",\n",
    "    \"    description = description_meta[\\\"content\\\"] if description_meta else None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    keywords_meta = soup.find(\\\"meta\\\", {\\\"name\\\": \\\"keywords\\\"})\\n\",\n",
    "    \"    keywords = keywords_meta[\\\"content\\\"] if keywords_meta else None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    internship_titles = [h3.text for h3 in soup.select(\\\"section.internship h3\\\")]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return title, description, keywords, internship_titles\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Function to save data to a CSV file\\n\",\n",
    "    \"def save_to_csv(data, filename):\\n\",\n",
    "    \"    with open(filename, mode=\\\"w\\\", newline=\\\"\\\", encoding=\\\"utf-8\\\") as file:\\n\",\n",
    "    \"        writer = csv.writer(file)\\n\",\n",
    "    \"        writer.writerow([\\\"Title\\\", \\\"Description\\\", \\\"Keywords\\\", \\\"Internship Titles\\\"])\\n\",\n",
    "    \"        writer.writerows(data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fetch the website content\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    html_content = fetch_url(url)\\n\",\n",
    "    \"    print(\\\"Website content fetched successfully\\\")\\n\",\n",
    "    \"    # Extract data if the website content was fetched successfully\\n\",\n",
    "    \"    if html_content:\\n\",\n",
    "    \"        title, description, keywords, internship_titles = extract_data(html_content)\\n\",\n",
    "    \"        data = [(title, description, keywords, \\\", \\\".join(internship_titles))]\\n\",\n",
    "    \"        save_to_csv(data, \\\"scraped_data.csv\\\")\\n\",\n",
    "    \"        print(\\\"Data extraction completed and saved to scraped_data.csv\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"Failed to fetch website content.\\\")\\n\",\n",
    "    \"except WebsiteAccessError as e:\\n\",\n",
    "    \"    print(f\\\"Error: {e}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending GET request to https://www.shadowfox.in/#\n",
    "Response status code: 200\n",
    "Website content fetched successfully\n",
    "Data extraction completed and saved to scraped_data.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
